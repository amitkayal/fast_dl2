{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10\n",
    "### NLP Classification and Translation\n",
    "\n",
    "We are going to look at NLP this week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Object Detection:\n",
    "\n",
    "- See Lesson 8 / Lesson 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Recall in image classification, with `conv_learner` it has a standard way to stick a layer ontop (any custom head) and have it do anything we like. Now we have flexibility to understand rotations, or solve other interesting problems. Lets take this concept and apply this to text.\n",
    "\n",
    "\n",
    "### NLP + Computer vision techniques Roadmap:\n",
    "\n",
    "- Learn to find **word structures** from **images** aka, **captioning**\n",
    "- Learn to find **images** to **word structures**\n",
    "- Learn to find **images** from **images**\n",
    "- Image enhancement - **upscaling low photos into high res**\n",
    "- **Segmenting** a picture into **objects** \n",
    "\n",
    "\n",
    "## `torchtext` to `fastai.text`\n",
    "\n",
    "`torchtext` is great, but its **slow**, it **doesn't run in parallel**, and it **doesn't remember**. **`fastai.text`** is a combination of `torchtext` and `fastai.nlp`. Consider `fastai.nlp` deprecated (outdated) at this point. \n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/uhTU35.jpg\" style=\"width:600px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from fastai.text import *\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB - International Movie Database\n",
    "\n",
    "[weblink to python notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/imdb.ipynb)\n",
    "\n",
    "#### Get the dataset\n",
    "\n",
    "```\n",
    "!wget http://files.fast.ai/data/aclImdb.tgz\n",
    "!tar -xf aclImdb.tgz\n",
    "```\n",
    "\n",
    "#### Check the file directories:\n",
    "```\n",
    "!ls ~/data/aclImdb/\n",
    "```\n",
    "\n",
    "        ->imdbEr.txt  imdb.vocab\tREADME\ttest  train\n",
    "\n",
    "#### Check the file counts\n",
    "\n",
    "```\n",
    "!ls /home/paperspace/data/aclImdb/train/all | wc -l\n",
    "75000\n",
    "```\n",
    "\n",
    "```\n",
    "!ls /home/paperspace/data/aclImdb/test/all | wc -l\n",
    "25000\n",
    "```\n",
    "\n",
    "#### Look at directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdbEr.txt  imdb.vocab\tmodels\tREADME\ttest  tmp  train\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/paperspace/data/aclImdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\t\t neg  unsup\t     urls_neg.txt  urls_unsup.txt\r\n",
      "labeledBow.feat  pos  unsupBow.feat  urls_pos.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/paperspace/data/aclImdb/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `CLAS` - classification and `LM` - language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create paths to save future features\n",
    "CLAS_PATH=Path('/home/paperspace/data/imdb_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "LM_PATH=Path('/home/paperspace/data/imdb_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "\n",
    "PATH=Path('/home/paperspace/data/aclImdb/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Text into Numbers\n",
    "\n",
    "- How do we turn sentences into numbers?\n",
    "\n",
    "### First lets prepare the `imdb` dataset, by creating a panda dataframe from the collection of `.txt` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(path):\n",
    "    \"\"\"\n",
    "    This function will go through the aclImdb folder\n",
    "    and create the necessary datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # initializes the text and labels collections\n",
    "    texts,labels = [],[]\n",
    "\n",
    "    \n",
    "    # for each sentiment\n",
    "    for idx,label in enumerate(CLASSES):\n",
    "    \n",
    "    \n",
    "        # will go through the \n",
    "        for fname in (path/label).glob('*.*'):\n",
    "    \n",
    "    \n",
    "            # open the file and append the filetext\n",
    "            texts.append(fname.open('r').read())\n",
    "    \n",
    "    \n",
    "            # open \n",
    "            labels.append(idx)\n",
    "    return np.array(texts),np.array(labels)\n",
    "\n",
    "\n",
    "trn_texts,trn_labels = get_texts(PATH/'train')\n",
    "val_texts,val_labels = get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at a sample of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " \"Basically, Cruel Intentions 2 is Cruel Intentions 1, again, only poorly done. The story is exactly the same as the first one (even some of the lines), with only a few exceptions. The cast is more unknown, and definitely less talented. Instead of being seductive and drawing me into watching it, I ended up feeling dirty because it compares to watching a soft-core porn. I'm not sure whether to blame some of the idiotic lines on the actors or the writers...and I always feel bad saying that, because I know how hard it is to do both...but it was basically a two-hour waste of my life. It literally amazes me that some movies get made, and this is no exception...I can't believe they'd make a third one.\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_labels[0],trn_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['labels','text']\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# shuffle the indexes in place\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))\n",
    "\n",
    "\n",
    "#shuffle texts\n",
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "\n",
    "\n",
    "#shuffle the labels \n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = val_labels[val_idx]\n",
    "\n",
    "\n",
    "#create dataframe\n",
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)\n",
    "\n",
    "\n",
    "# saving training and validation dataset\n",
    "df_trn[df_trn['labels']!=2].to_csv(CLAS_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(CLAS_PATH/'test.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "#write the classes\n",
    "(CLAS_PATH/'classes.txt').open('w').writelines(f'{o}\\n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create trn and validation texts with different test size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trn_texts,val_texts = train_test_split(np.concatenate([trn_texts,val_texts]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Language model `LM` dataframes and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)}, columns=col_names)\n",
    "\n",
    "df_trn.to_csv(LM_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH/'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Tokens\n",
    "\n",
    "- **tokenization** - turn a sentence into words with some specific rules, correcting for puncutation.\n",
    "- **`fixup()`** - there's always strange encodings in text, here are a couple that will replace some outlier characters\n",
    "- **`get_texts()`** - will iterate through all files and collect the in-file text\n",
    "- **`get_all()`** - will call `get_texts()` repeatedly for each row of the dataframe, pulling text from the source text files and returning tokens and labels\n",
    "\n",
    "\n",
    "### Reference\n",
    "- **`Tokenizer().proc_all_mp(partition_by_cores(texts))`**\n",
    "\n",
    "**proc_all_mp** function\n",
    "\n",
    "```python\n",
    "def proc_all_mp(ss, lang='en'):\n",
    "        ncpus = num_cpus()//2\n",
    "        with ProcessPoolExecutor(ncpus) as e:\n",
    "            return sum(e.map(Tokenizer.proc_all, ss, [lang]*len(ss)), [])\n",
    "```\n",
    "\n",
    "**partition_by_cores** function\n",
    "\n",
    "```python\n",
    "def partition_by_cores(a):\n",
    "    return partition(a, len(a)//num_cpus() + 1)\n",
    "```\n",
    "\n",
    "**`ProcessPoolExecutor` is from a Python 3 standard library**:\n",
    "[More on multiprocessing](https://docs.python.org/3/library/concurrent.futures.html)\n",
    "```python\n",
    "# generalized use of ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "def main():\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):\n",
    "            print('%d is prime: %s' % (number, prime))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "when given to pandas, it won't return a full dataframe, \n",
    "but it will return an iterator. It will return sub-sized chunks\n",
    "over and over again\n",
    "\"\"\"\n",
    "chunksize=24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    \"\"\" Cleans up erroroneus characters\"\"\"\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    \n",
    "    # pull the labels out from the dataframe\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    \n",
    "    # pull the full FILEPATH for each text\n",
    "    # BOS is a flag to indicate when a new text is starting\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    \n",
    "    # Sometimes, text has title, or other sub-sections. We will record all of these\n",
    "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "\n",
    "    # Tokenize the data\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Spacy if you don't have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.7 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: six in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: html5lib==1.0b8 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: pathlib in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: ujson>=1.35 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: regex==2017.4.5 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: msgpack-python==0.5.4 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.1 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: wrapt in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: termcolor in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already satisfied: wcwidth in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download English Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 6.7MB/s ta 0:00:0111\n",
      "\u001b[?25h  Requirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in our Language model data, extract texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " __spec__ = \"ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(LM_PATH/'test.csv', header=None, chunksize=chunksize)\n",
    "\n",
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make working `tmp` directory & save tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenization & check top tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')\n",
    "\n",
    "freq = Counter(p for o in tok_trn for p in o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1208449),\n",
       " ('.', 992545),\n",
       " (',', 986614),\n",
       " ('and', 587567),\n",
       " ('a', 583520),\n",
       " ('of', 525412),\n",
       " ('to', 484871),\n",
       " ('is', 393923),\n",
       " ('it', 341485),\n",
       " ('in', 337351),\n",
       " ('i', 307751),\n",
       " ('this', 270410),\n",
       " ('that', 261107),\n",
       " ('\"', 237920),\n",
       " (\"'s\", 222037),\n",
       " ('-', 188209),\n",
       " ('was', 180235),\n",
       " ('\\n\\n', 179009),\n",
       " ('as', 166145),\n",
       " ('with', 159253),\n",
       " ('for', 158601),\n",
       " ('movie', 157735),\n",
       " ('but', 150659),\n",
       " ('film', 144618),\n",
       " ('you', 123979)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recreate a single entry\n",
    "\n",
    "- **`xbos`** - start stream\n",
    "- **`xfld`** - start field\n",
    "- **word on caps** - how do get the semantic impact of CAPS vs. normal version? \n",
    "- **`t_up`** - add a tag in front to indicate the next word is uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n xbos xfld 1 i saw this movie at the dragon*con 2006 independent film festival . it was awarded 2 awards at that festival and rightfully so . this is probably the best short horror film i \\'ve ever seen . the simplicity of camera usage really works.the main character is brilliant . his acting is quite good and is believable . the 3 cameras in the room with tim russel make his insanity that much more believable . i love it . i have talked with mike and he says that they are in the process of making a feature film compassing the first three chapters together . i ca n\\'t wait . i will be first in line for that film . the effects of the \" mirror \" creatures are used so well . you do n\\'t see them for very long so it scares the pants off of you when you do . i recommend this film to anyone who wants to watch a good horror movie for once . best 32 minutes of spine tingling horror i \\'ve ever seen . thanks mike .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tok_trn[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Vocabulary and add a term for unknown and for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60002\n"
     ]
    }
   ],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2\n",
    "\n",
    "\n",
    "# index to word\n",
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "\n",
    "\n",
    "# word to index\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "print(len(itos))\n",
    "\n",
    "\n",
    "# create a array of token_indices \n",
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "\n",
    "# save the i\n",
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40 41 42 39 12 235 13 23 44 2 0 3368 1662 25 1331 3 10 18 8833 261 2474 44 14 1331 5 9383 51 3 13 9 263 2 138 364 200 25 12 159 143 129 3 2 5416 7 371 9080 83 58696 305 122 9 556 3 35 136 9 204 66 5 9 842 3 2 379 3992 11 2 655 21 1895 13095 113 35 4853 14 93 67 842 3 12 133 10 3 12 36 3515 21 1555 5 34 566 14 45 33 11 2 1648 7 251 6 820 25 0 2 105 299 8000 312 3 12 196 29 881 3 12 104 37 105 11 367 22 14 25 3 2 306 7 2 15 3006 15 2204 33 345 51 88 3 26 57 29 82 111 22 69 216 51 10 2702 2 3719 141 7 26 68 26 57 3 12 404 13 25 8 273 48 505 8 126 6 66 200 23 22 301 3 138 13558 249 7 7191 21684 200 12 159 143 129 3 1179 1555 3'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([str(val) for val in trn_lm[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60002, 90000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))\n",
    "\n",
    "vs=len(itos)\n",
    "vs,len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instead of pretraining on imagenet, for NLP we will use a large subset of wikipedia\n",
    "\n",
    "<img src='https://snag.gy/7VlF1t.jpg' style='width:600px' />\n",
    "\n",
    "Previously in lesson 4, we trained a language model that was state of the art. Using a pre-selected text articles from wikipedia, a language model was trained, and the weights were saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikitext103 model - weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! wget -nH -r -np -P {PATH} http://files.fast.ai/models/wt103/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use a pretrained model - must have same network sizes\n",
    "- **`em_sz`** -  embedding sizes for vectors (400)\n",
    "- **`nh`** - number of hidden  (# of activations)\n",
    "- **`nl`** - number of layers (hidden)\n",
    "\n",
    "- **model type: AWD LSTM** [link](https://github.com/salesforce/awd-lstm-lm)\n",
    "\n",
    "Some work will have to be done to map the **pre-trained vocabulary** to the **current vocab** that we are working with. Any  words not found in pre-trained vocab, we will use global *mean* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "# set filepaths\n",
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'\n",
    "\n",
    "\n",
    "# load weights (returns a dictionary)\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)\n",
    "\n",
    "\n",
    "# pull out embedding weights\n",
    "# sized vocab x em_sz \n",
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "\n",
    "\n",
    "# load pre trained vocab to index mappings\n",
    "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})\n",
    "\n",
    "\n",
    "# create a pre-trained -> current corpus vocab to vocab mapping\n",
    "# initialize an empty matrix\n",
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "\n",
    "\n",
    "# loop through by row index and insert the correct embedding\n",
    "for i,w in enumerate(itos):\n",
    "    r = stoi2[w]\n",
    "    new_w[i] = enc_wgts[r] if r>=0 else row_m\n",
    "\n",
    "    \n",
    "# create our torch `state` that we will load later\n",
    "wgts['0.encoder.weight'] = T(new_w)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Creation\n",
    "\n",
    "- **`wd`** - weight decay\n",
    "- **`bptt`** - back prop through time\n",
    "- **`bs`** - batchsize\n",
    "\n",
    "We will be performing a continuous process of \n",
    "\n",
    "        given words --> predict next word\n",
    "\n",
    "### Lesson 4 - note the best loss\n",
    "    [ 0.      4.3926  4.2917]                                       \n",
    "    [ 1.       4.37693  4.28255]                                  \n",
    "    [ 2.       4.37998  4.27243]                                  \n",
    "    [ 3.       4.34284  4.24789]                                  \n",
    "    [ 4.      4.3287  4.2317]                                     \n",
    "    [ 5.       4.28881  4.20722]                                  \n",
    "    [ 6.       4.24637  4.18926]                                  \n",
    "    [ 7.       4.23797  4.17644]  \n",
    "\n",
    "\n",
    "### Pretrained Model - we already start with a better score\n",
    "    epoch      trn_loss   val_loss   accuracy                     \n",
    "        0      4.332359   4.120674   0.289563  \n",
    "        1      4.247177   4.067932   0.294281 \n",
    "        \n",
    "        \n",
    "### Comparison to `Word2Vec`\n",
    "\n",
    "**`Word2Vec`** - single embedding matrix. Each word has a matrix and thats it. It's a **single layer** (input) from a pretrained model. It's from a **linear model** on a **co-occurance matrix**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)\n",
    "\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Components:\n",
    "\n",
    "## 1. DataLoader\n",
    "## 2. ModelData\n",
    "## 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Language Model Loader\n",
    "\n",
    "<img src='https://snag.gy/Fb0e6S.jpg' style='width:600px' />\n",
    "\n",
    "### Key Notes:\n",
    "\n",
    "- **Batch size** is not the count of things in a batch. In this context, batch_size is more **batch_ct**\n",
    "- **Randomness-> size instead of order** - if we grab 70 at a time, and do a new epoch, the data will be exactly the same, and all the batches will be identical. In images we would shuffle, but that doesn't work in the language model, because it is trying to learn the sentence. So if order can't change, let's **randomly change the sequence length**\n",
    "\n",
    "```python\n",
    "class LanguageModelLoader():\n",
    "    \"\"\"Returns tuples of mini-batches.\"\"\"\n",
    "    \n",
    "    def __init__(self, nums, bs, bptt, backwards=False):\n",
    "        \n",
    "        # assign values\n",
    "        self.bs,self.bptt,self.backwards = bs,bptt,backwards\n",
    "        \n",
    "        # batchify the numbers. Based on the batchsize\n",
    "        # subdivide the data. Note: batchsize 64\n",
    "        # 640,000 would be broken into 64 x 10,000 \n",
    "        self.data = self.batchify(nums)\n",
    "        \n",
    "        # initialize other values\n",
    "        self.i,self.iter = 0,0\n",
    "        self.n = len(self.data)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Iterator implementation\"\"\"\n",
    "            \n",
    "        # start from zero\n",
    "        self.i,self.iter = 0,0\n",
    "            \n",
    "        # will continually pull data out\n",
    "        while self.i < self.n-1 and self.iter<len(self):\n",
    "            if self.i == 0:\n",
    "                seq_len = self.bptt + 5 * 5\n",
    "            else:\n",
    "                bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n",
    "                seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
    "            res = self.get_batch(self.i, seq_len)\n",
    "            self.i += seq_len\n",
    "            self.iter += 1\n",
    "                        \n",
    "            # yields the value\n",
    "            yield res\n",
    "\n",
    "                        \n",
    "    def __len__(self): return self.n // self.bptt - 1\n",
    "\n",
    "                        \n",
    "    def batchify(self, data):\n",
    "        \"\"\"splits the data into batch_size counts of sets\"\"\"\n",
    "        nb = data.shape[0] // self.bs\n",
    "        data = np.array(data[:nb*self.bs])\n",
    "        data = data.reshape(self.bs, -1).T\n",
    "        if self.backwards: data=data[::-1]\n",
    "                                \n",
    "        # returns the transpose\n",
    "        # have batch_size number of columns \n",
    "        return T(data)\n",
    "\n",
    "    def get_batch(self, i, seq_len):\n",
    "        source = self.data\n",
    "        seq_len = min(seq_len, len(source) - 1 - i)\n",
    "        return source[i:i+seq_len], source[i+1:i+1+seq_len].view(-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelData\n",
    "\n",
    "<img src='https://snag.gy/y4z2w1.jpg' style='width:600px' />\n",
    "\n",
    "```python\n",
    "\n",
    "class LanguageModelData():\n",
    "    \"\"\"\n",
    "    - a training data loader\n",
    "    - a validation data loader\n",
    "    - a test loader\n",
    "    - a saving path\n",
    "    - model parameteres\n",
    "    \"\"\"\n",
    "    def __init__(self, path, pad_idx, nt, trn_dl, val_dl, test_dl=None, bptt=70, backwards=False, **kwargs):\n",
    "        self.path,self.pad_idx,self.nt = path,pad_idx,nt\n",
    "        self.trn_dl,self.val_dl,self.test_dl = trn_dl,val_dl,test_dl\n",
    "\n",
    "    def get_model(self, opt_fn, emb_sz, n_hid, n_layers, **kwargs):\n",
    "        m = get_language_model(self.nt, emb_sz, n_hid, n_layers, self.pad_idx, **kwargs)\n",
    "        model = LanguageModel(to_gpu(m))\n",
    "        return RNN_Learner(self, model, opt_fn=opt_fn)\n",
    "```\n",
    "\n",
    "### Our Language Model extends the `basic model`\n",
    "\n",
    "We are overriding one method that returns a list of all your layer groups.\n",
    "\n",
    "```python\n",
    "class LanguageModel(BasicModel):\n",
    "    def get_layer_groups(self):\n",
    "        m = self.model[0]\n",
    "        return [*zip(m.rnns, m.dropouths), (self.model[1], m.dropouti)]\n",
    "```\n",
    "\n",
    "\n",
    "### Extend the `learner class`, set default to cross entropy\n",
    "```python\n",
    "class RNN_Learner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "        self.crit = F.cross_entropy\n",
    "\n",
    "    def save_encoder(self, name): save_model(self.model[0], self.get_model_path(name))\n",
    "    def load_encoder(self, name): load_model(self.model[0], self.get_model_path(name))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of the RNN Encoder\n",
    "\n",
    "[AWD Paper](https://arxiv.org/pdf/1708.02182)\n",
    "\n",
    "[Link to the github](https://github.com/fastai/fastai/blob/master/fastai/lm_rnn.py)\n",
    "\n",
    "- **Embedding input layer**\n",
    "- **1 x LSTM layer per layer asked for `nl`**\n",
    "- **The rest are places to put dropout**\n",
    "\n",
    "### Forward:\n",
    "- **Call the Embedding Layer**\n",
    "- **Add some drop out**\n",
    "- **Call RNN layer**\n",
    "- **Append to outputs**\n",
    "- **drop out**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN_Encoder(nn.Module):\n",
    "\n",
    "    \"\"\"A custom RNN encoder network that uses\n",
    "        - an embedding matrix to encode input,\n",
    "        - a stack of LSTM layers to drive the network, and\n",
    "        - variational dropouts in the embedding and LSTM layers\n",
    "        The architecture for this network was inspired by the work done in\n",
    "        \"Regularizing and Optimizing LSTM Language Models\".\n",
    "        (https://arxiv.org/pdf/1708.02182.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, ntoken, emb_sz, nhid, nlayers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5):\n",
    "        \"\"\" Default constructor for the RNN_Encoder class\n",
    "            Args:\n",
    "                bs (int): batch size of input data\n",
    "                ntoken (int): number of vocabulary (or tokens) in the source dataset\n",
    "                emb_sz (int): the embedding size to use to encode each token\n",
    "                nhid (int): number of hidden activation per LSTM layer\n",
    "                nlayers (int): number of LSTM layers to use in the architecture\n",
    "                pad_token (int): the int value used for padding text.\n",
    "                dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "                dropouti (float): dropout to apply to the input layer.\n",
    "                dropoute (float): dropout to apply to the embedding layer.\n",
    "                wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "            Returns:\n",
    "                None\n",
    "          \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.ndir = 2 if bidir else 1\n",
    "        self.bs = 1\n",
    "        self.encoder = nn.Embedding(ntoken, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_with_dropout = EmbeddingDropout(self.encoder)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else nhid, (nhid if l != nlayers - 1 else emb_sz)//self.ndir,\n",
    "             1, bidirectional=bidir, dropout=dropouth) for l in range(nlayers)]\n",
    "        if wdrop: self.rnns = [WeightDrop(rnn, wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "\n",
    "        self.emb_sz,self.nhid,self.nlayers,self.dropoute = emb_sz,nhid,nlayers,dropoute\n",
    "        self.dropouti = LockedDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([LockedDropout(dropouth) for l in range(nlayers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" Invoked during the forward propagation of the RNN_Encoder module.\n",
    "        Args:\n",
    "            input (Tensor): input of shape (sentence length x batch_size)\n",
    "        Returns:\n",
    "            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\n",
    "            dropouth, list of tensors evaluated from each RNN layer using dropouth,\n",
    "        \"\"\"\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "\n",
    "        emb = self.encoder_with_dropout(input, dropout=self.dropoute if self.training else 0)\n",
    "        emb = self.dropouti(emb)\n",
    "\n",
    "        raw_output = emb\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "            current_input = raw_output\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.nlayers - 1: raw_output = drop(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "\n",
    "        self.hidden = repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "        nh = (self.nhid if l != self.nlayers - 1 else self.emb_sz)//self.ndir\n",
    "        return Variable(self.weights.new(self.ndir, self.bs, nh).zero_(), volatile=not self.training)\n",
    "\n",
    "    def reset(self):\n",
    "        self.weights = next(self.parameters()).data\n",
    "        self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.nlayers)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Dropout\n",
    "\n",
    "If you have less data for your language model, you will need more dropout. If you have more data, you will need less dropout. Otherwise the following dropout numbers were selected by experimentation:\n",
    "\n",
    "```python\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "```\n",
    "\n",
    "**Note on : 0.7** if you are overfitting, increase the number, if you are underfitting, decrease this number\n",
    "\n",
    "Normally we look at **cross-entropy loss**. But comparing CE Loss, if you are right, you should be very confident. Accuracy only cares if the answer was right or wrong, often times is more stable to track.\n",
    "\n",
    "## Back to the Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)\n",
    "\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note to reader - this block takes a long time to run (single epoch) - 30mins/epoch on 8 core 32 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81b96b3a6e5437b94109d6d0bfcdff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                     \n",
      "    0      4.354009   4.18011    0.285487  \n",
      "\n",
      "time to train 1 epoch, 1856.5503034591675\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "learner.model.load_state_dict(wgts)\n",
    "\n",
    "# fit a single cycle\n",
    "lr=1e-3\n",
    "lrs = lr\n",
    "start = time.time()\n",
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)\n",
    "print(\"time to train 1 epoch,\", time.time()-start)\n",
    "\n",
    "learner.save('lm_last_ft')\n",
    "learner.load('lm_last_ft')\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this will take a while to run - 15 epochs! make sure you have the time / computing resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for a learning rate, then run for 15 epoches\n",
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)\n",
    "learner.sched.plot()\n",
    "\n",
    "start = time.time()\n",
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)\n",
    "print(\"Time to train,\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After fitting the model we save it\n",
    "\n",
    "We save the trained model weights and separately save the encoder part of the LM model as well. This will serve as our backbone in the classification task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the model\n",
    "learner.save('lm1')\n",
    "\n",
    "# saves just the RNN encoder (rnn_enc)\n",
    "learner.save_encoder('lm1_enc')\n",
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From FASTAI reference notebook - Results of the 15 epoches\n",
    "\n",
    "    epoch      trn_loss   val_loss   accuracy\n",
    "        0      4.332359   4.120674   0.289563\n",
    "        1      4.247177   4.067932   0.294281\n",
    "        2      4.175848   4.027153   0.298062\n",
    "        3      4.140306   4.001291   0.300798\n",
    "        4      4.112395   3.98392    0.302663\n",
    "        5      4.078948   3.971053   0.304059\n",
    "        6      4.06956    3.958152   0.305356\n",
    "        7      4.025542   3.951509   0.306309\n",
    "        8      4.019778   3.94065    0.30756 \n",
    "        9      4.027846   3.931385   0.308232\n",
    "        10     3.98106    3.928427   0.309011\n",
    "        11     3.97106    3.920667   0.30989 \n",
    "        12     3.941096   3.917029   0.310515\n",
    "        13     3.924818   3.91302    0.311015\n",
    "        14     3.923296   3.908476   0.311586"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Classifier Tokens\n",
    "\n",
    "The classifier model is basically a linear layer custom head on top of the LM backbone. Setting up the classifier data is similar to the LM data setup except that we cannot use the unsup movie reviews this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data again\n",
    "df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "df_val = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=chunksize)\n",
    "\n",
    "\n",
    "# get the tokens\n",
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
    "np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')\n",
    "\n",
    "\n",
    "# We load the integer to vocab that we saved before\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "# create all matricies with indices\n",
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "# then save the matricies\n",
    "np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\n",
    "np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "Now we can create our final model, a classifier which is really a custom linear head over our trained IMDB backbone. The steps to create the classifier model are similar to the ones for the LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load our numpy arrays with indexes (representing vocab)\n",
    "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids.npy')\n",
    "val_clas = np.load(CLAS_PATH/'tmp'/'val_ids.npy')\n",
    "\n",
    "# we load our labels\n",
    "trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels.npy'))\n",
    "\n",
    "\n",
    "# set up our model parameters\n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "\n",
    "# select our optimizer\n",
    "# also pick a batch size as big as you can that doesn't run out of memory\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48\n",
    "\n",
    "min_lbl = trn_labels.min()\n",
    "trn_labels -= min_lbl\n",
    "val_labels -= min_lbl\n",
    "c=int(trn_labels.max())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the classifier, unlike LM, we need to read a movie review at a time and learn to predict the it's sentiment as pos/neg. We do not deal with equal bptt size batches, so we have to pad the sequences to the same length in each batch. To create batches of similar sized movie reviews, we use a sortish sampler method invented by [@Smerity](https://twitter.com/Smerity) and [@jekbradbury](https://twitter.com/jekbradbury)\n",
    "\n",
    "The sortishSampler cuts down the overall number of padding tokens the classifier ends up seeing.\n",
    "\n",
    "### Note: If documents are different lengths, they should be padded to be the same size. Luckily `fastai` does this automatically\n",
    "<img src='https://snag.gy/EgBco0.jpg' style='width:600px' />\n",
    "\n",
    "### Process Optimizing Note: Put the short documents first (with some randomness)\n",
    "\n",
    "<img src='https://snag.gy/fIBawX.jpg' style='width:600px' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create basic text datasets\n",
    "trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "val_ds = TextDataset(val_clas, val_labels)\n",
    "\n",
    "\n",
    "# sort the docs based on size. \n",
    "# validation will be explicitly short -> long\n",
    "# training, which sorts loosely\n",
    "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "\n",
    "# then we create our dataloaders as before but with a [sampler] parameter\n",
    "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "md = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our Pretrained Model, and train the last Layer\n",
    "\n",
    "### We will pass hidden layer details\n",
    "\n",
    "```python\n",
    "layers=[em_sz*3, 50, c]\n",
    "```\n",
    "- **`em_sz*3`** - inputsize\n",
    "- **`50`** - output of first layer\n",
    "- **`c`** - output of the 2nd layer\n",
    "\n",
    "### Why x3? - Concat pooling\n",
    "\n",
    "We take the average pooling over the sequence, the max pooling, and the final pooling and concatenating them all together\n",
    "\n",
    "<img src='https://snag.gy/xpW1Ft.jpg' style='width:600px' />\n",
    "\n",
    "### pass in drop out details\n",
    "\n",
    "```python\n",
    "drops=[dps[4], 0.1]\n",
    "```\n",
    "\n",
    "### pass the AWD dropout parameters:\n",
    "```python\n",
    "dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup our dropout rates\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5\n",
    "m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "# define our RNN learner\n",
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=25.\n",
    "learn.metrics = [accuracy]\n",
    "\n",
    "# set our learning rate\n",
    "# we will use discriminative learning rates for different layers\n",
    "lr=3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "\n",
    "# Now we load our language model from before\n",
    "# but freeze everything except the last layer\n",
    "lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "wd = 1e-7\n",
    "wd = 0\n",
    "learn.load_encoder('lm2_enc')\n",
    "learn.freeze_to(-1)\n",
    "\n",
    "# find the optimal learning rate\n",
    "learn.lr_find(lrs/1000)\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the last Layer\n",
    "\n",
    "```\n",
    "A Jupyter Widget\n",
    "epoch      trn_loss   val_loss   accuracy                      \n",
    "    0      0.365457   0.185553   0.928719  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "learn.save('clas_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we freeze everything except the last two layers?\n",
    "```\n",
    "epoch      trn_loss   val_loss   accuracy                      \n",
    "    0      0.340473   0.17319    0.933125 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_0')\n",
    "learn.freeze_to(-2)\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "learn.save('clas_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets try and train the whole model\n",
    "\n",
    "Note that the state of the art is 0.941, which is beaten in 3-4 epoches\n",
    "```\n",
    "epoch      trn_loss   val_loss   accuracy                      \n",
    "    0      0.337347   0.186812   0.930782  \n",
    "    1      0.284065   0.318038   0.932062                      \n",
    "    2      0.246721   0.156018   0.941747                      \n",
    "    3      0.252745   0.157223   0.944106                      \n",
    "    4      0.24023    0.159444   0.945393                      \n",
    "    5      0.210046   0.202856   0.942858                      \n",
    "    6      0.212139   0.149009   0.943746                      \n",
    "    7      0.21163    0.186739   0.946553                      \n",
    "    8      0.186233   0.1508     0.945218                      \n",
    "    9      0.176225   0.150472   0.947985                      \n",
    "    10     0.198024   0.146215   0.948345                      \n",
    "    11     0.20324    0.189206   0.948145                      \n",
    "    12     0.165159   0.151402   0.947745                      \n",
    "    13     0.165997   0.146615   0.947905 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_1')\n",
    "learn.unfreeze()\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun thing to try: do the same thing, but reverse the document\n",
    "\n",
    "When you combine the forward and backward models and average the results, we get a 95% accuracy!\n",
    "The previous state of the art result was 94.1% accuracy (5.9% error). With bidir we get 95.4% accuracy (4.6% error).\n",
    "\n",
    "### Check out the paper\n",
    "[link](https://arxiv.org/pdf/1801.06146)\n",
    "\n",
    "<img src='https://snag.gy/T1gxzK.jpg' style='width:600px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai.text performance on text sets\n",
    "\n",
    "<img src='https://snag.gy/SIyvW0.jpg' style='width:600px' />\n",
    "\n",
    "## discriminative learning rates - renamed from differential learning rates\n",
    "\n",
    "<img src='https://snag.gy/JmH09e.jpg' style='width:600px' />\n",
    "\n",
    "## gradual unfreezing learning rates - unfreezing a layer at a time\n",
    "\n",
    "<img src='https://snag.gy/9kgDUj.jpg' style='width:600px' />\n",
    "\n",
    "## Tweaked approach to cyclical learning rates CLR - based on idea from Leslie smith\n",
    "\n",
    "Only do 1 cycle that goes up quickly and goes down slower afterwards. Currently implemented in fastai. First number is ratio of highest learning rate to the lowest ratio rate. **32**. Second number is the ratio between teh first peak and the last peak. First epoch to be upward, and 9 down, = **10** \n",
    "\n",
    "[link to the paper](https://arxiv.org/pdf/1803.09820)\n",
    "\n",
    "<img src='https://snag.gy/9u1I6i.jpg' style='width:600px' />\n",
    "\n",
    "## BPTT - Normal RNN vs. MultiBatch RNN:\n",
    "\n",
    "Key difference is that hte normal RNN encoder. We can BPTT chunk at a time, and predice the next word. But for the classifier, we need to do the entire doc, or the entire movie review. The entire review could be 2000 words, and we can't fit all in memory.\n",
    "\n",
    "Sends back only as many activations as we have decided to keep. If your max is 1000, but your doc is 2000. It will go through and only keep the most recent 1000 activations. \n",
    "\n",
    "<img src='https://snag.gy/Z068zb.jpg' style='width:600px' />\n",
    "\n",
    "## Results vs. customized algorithms\n",
    "\n",
    "<img src='https://snag.gy/EJkOTo.jpg' />\n",
    "\n",
    "## how do the different techniques affect the score? \n",
    "\n",
    "- What happens with different dataset sizes\n",
    "- What happens if different techniques are turned on and off?\n",
    "\n",
    "<img src='https://snag.gy/zWLsgp.jpg' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
