{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Models\n",
    "\n",
    "### Machine translation -> Neural Translation\n",
    "\n",
    "<img src='https://snag.gy/2aJ8PI.jpg' style='width:500px'>\n",
    "\n",
    "- 2015 : neural translation first appeared and wasn't that performant compared to traditional statistical approach\n",
    "- 2016 : neural translation outperforms other models\n",
    "- Neural translation now on track similar to image recognition\n",
    "\n",
    "**Why a machine translation model if google has a translate service?** - The reason is, the general idea of taking some input say french sentence, and outputing something of arbitrary length (say an english sentence) is very useful. Another example is taking in text and returning a summary.\n",
    "\n",
    "<img src='https://snag.gy/39kJgU.jpg' style='width:500px'>\n",
    "\n",
    "**Seq2Seq Benefits**:\n",
    "- We don't have to fiddle with different options or parameters or feature engineering\n",
    "- Distributed representation\n",
    "- long-term state (instead of n-gram approaches)\n",
    "- Text generation is more fluent\n",
    "\n",
    "<img src='https://snag.gy/tGSWiB.jpg' style='width:500px'>\n",
    "\n",
    "-  Bi-directional LSTM with attention \n",
    "\n",
    "<img src='https://snag.gy/Fl0Mho.jpg' style='width:500px'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our process\n",
    "\n",
    "<img src='https://snag.gy/RcH7Tm.jpg' width='500px'>\n",
    "\n",
    "#### Previously\n",
    "1. we had a decoder that took the RNN,\n",
    "2. took a MaxPool over all time\n",
    "3. MeanPool over all time\n",
    "4. value of RNN at last time step and put it through a linear layer\n",
    "\n",
    "#### Going forward\n",
    "1. we start a sentence in english\n",
    "2. put it through an RNN\n",
    "3. *Language Model was easier because the number of output tokens corresponds exactly to the input tokens\n",
    "4. The tokens in the output will NOT correspond directly to the tokens in the input\n",
    "5. RNN encode input -- > hidden state --> generate output sequence\n",
    "6. We know seq -- > class (sentiment classifier)\n",
    "7. We know seq -- > exact sequence  (language model)\n",
    "8. Now seq --> seq\n",
    "\n",
    "\n",
    "### Quick Review of RNN\n",
    "\n",
    "<img src='https://snag.gy/TDBvrk.jpg' width='500px'>\n",
    "\n",
    "1. RNN is a fully connected network (4 layers in the diagram)\n",
    "2. At each layer, it concatenates additional inputs\n",
    "3. This can be accomplished with linear layers and ReLus.\n",
    "4. Then we simplify the diagram, and made a loop as follows:\n",
    "\n",
    "<img src='https://snag.gy/IQwAfx.jpg' width='500px'>\n",
    "\n",
    "<img src='https://snag.gy/3f7Uey.jpg' width='500px'>\n",
    "\n",
    "#### note that the hidden state `h` in this example is a vector\n",
    "\n",
    "\n",
    "### Stacking RNNs\n",
    "\n",
    "<img src='https://snag.gy/salbhZ.jpg' style='width:500px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
